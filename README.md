# SIGNATE: 2025 MUFG

## 結果: 31位/89名

## 取り組み
- 前処理
  - 特徴量エンジニアリング
    - 数値
      - 四則演算による特徴量の新規作成
    - 日付
      - そのまま利用＋四半期分割
  - テキスト処理
    - `notes/n99.ipynb`参照
- モデル選択
  - のちのアンサンブルでモデル間の多様性を高めるため、以下を利用
    - ツリー系
      - lightGBM, XGBoost, CatBoost
        - lightGBM では、`DART`(Dropouts meet Multiple Additive Regression Trees)と`GBDT`（勾配ブースティング決定木）の併用により精度向上
    - ニューラルネット系
      - sklearn.MLP, PyTorch, RealMLP
        - PyTorch ではEmbedding層を実装し、カテゴリカル変数の効果的利用を試みた
    - その他
      - ロジスティックモデル＋特徴量サブサンプリング
      - k近傍法
- モデルアンサンブル
  - OOF予測値を利用したスタッキング
    - trainデータとtestデータを想定
    - k-fold でtrainデータをt-train, t-val に分割（１＜t＜k）
    - t-trainでモデル（**ベースモデル**）の学習
    - ベースモデルでt-val, testを予測し、その予測値を新しい特徴量とする（**メタ特徴量**）
    - メタ特徴量を含めて新しいモデル（**メタモデル**）で学習
  - ベースモデル、メタモデルに上記のモデルを利用
  - 多重線形性回避のため、メタ特徴量は主成分化した
- 総じて
  - Gemini CLI の利用
    - ipynbの編集はできなかった
    - 友達に教えてもらった、readme.md を作成して、それを元にスクリプトを作成させる、という手法
    - コンセプトを自分で深掘りできて、思考の整理になった
    - 途中から面倒になってreadmeを更新しなくなった
    - コードが冗長になりがちなので、叩き台を作成させてから自分で編集するのが吉
  - ディレクトリ構成・命名規則
    - `scripts`, `output`, `notes` の構成はやりやすかった。でも`notes`も通し番号＋慣用名 にしよう
    - 実行ファイル群は`scripts/shell` とかで分割した方が良いかも
    - クラスオブジェクトのモジュール
      - `f01__hogehoge()`のように通し番号で管理
      - ヘルパ関数は`_hogehoge()`
  - ライブラリのインストール
    - `pip install` でインストールすると、`CatBoost`のインストール時に他のライブラリと衝突した
    - `conda install`でインストールしたら、スムーズにインストールできたし衝突もなかった